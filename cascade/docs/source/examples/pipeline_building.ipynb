{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline building\n",
    "This use-case is pipeline building. For this example the task of classification on MNIST is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\илья\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import cascade.data as cdd\n",
    "import cascade.meta as cme\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cascade\n",
    "cascade.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_ROOT = 'data'\n",
    "\n",
    "train_ds = torchvision.datasets.MNIST(root=MNIST_ROOT,\n",
    "                                     train=True, \n",
    "                                     transform=F.to_tensor,\n",
    "                                     download=True)\n",
    "test_ds = torchvision.datasets.MNIST(root=MNIST_ROOT, \n",
    "                                    train=False, \n",
    "                                    transform=F.to_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding metadata\n",
    "But in the end we need not only loaded dataset, but the container for metadata we can store. So the next step is to wrap these datasets into Cascade's objects.\n",
    "  \n",
    "Suppose we also need to write some data description to be able to know on which data our model was trained.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = cdd.Wrapper(train_ds)\n",
    "train_ds.describe(\"This is MNIST dataset of handwritten images\")\n",
    "test_ds = cdd.Wrapper(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying noise\n",
    "Let's say we want to apply noise to an image.  \n",
    "*We will use hardcoded magnitude to simplify an example.*  \n",
    "To do this we need to make a Modifier. Modifier wraps another dataset and applies a function to its elements in a lazy way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseModifier(cdd.Modifier):\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self._dataset[index] # get the data from Wrapper, which is _dataset for this Modifier\n",
    "        img += torch.rand_like(img) * 0.1 # apply random noise with fixed magnitude\n",
    "        img = torch.clip(img, 0, 255)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NoiseModifier(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "Data validation is one of the most important parts of any ML-pipeline. Testing some assumtions is useful when our pipeline is very complex, but here let's add validation stage just for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cascade.meta.validator.PredicateValidator"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cme.PredicateValidator(train_ds, lambda x: torch.all(x[0] < 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versioning\n",
    "For tracking changes in the pipeline during the experiments, the version could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdd.version(train_ds, 'train_ds_version_log.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing metadata\n",
    "To view metadata of the pipeline we can see what `get_meta()` gives:  \n",
    "First is NoiseModifier and the second is Wrapper around MNIST Dataset with our custom description.  \n",
    "Using `update_meta()` we can add any info we want to the object's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '__main__.NoiseModifier',\n",
       "  'description': None,\n",
       "  'tags': [],\n",
       "  'comments': [],\n",
       "  'links': [],\n",
       "  'type': 'dataset',\n",
       "  'len': 60000},\n",
       " {'name': 'cascade.data.dataset.Wrapper',\n",
       "  'description': 'This is MNIST dataset of handwritten images',\n",
       "  'tags': [],\n",
       "  'comments': [],\n",
       "  'links': [],\n",
       "  'type': 'dataset',\n",
       "  'len': 60000,\n",
       "  'obj_type': \"<class 'torchvision.datasets.mnist.MNIST'>\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.get_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to train model\n",
    "Now we can set the batch size and pass our pipeline to the DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainldr = torch.utils.data.DataLoader(dataset=train_ds, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       shuffle=True)\n",
    "testldr = torch.utils.data.DataLoader(dataset=test_ds,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See also:\n",
    "[Train models](model_training.html)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cascade_full_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faab9700dd378963078e8736d2f2a2135ebae0340eb64481dd59710303e6f8c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
