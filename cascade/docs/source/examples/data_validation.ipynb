{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data validation\n",
    "   \n",
    "    \n",
    "This use-case is about validation of the data before model training.  \n",
    "The scenarios are numerous and by using Cascade you can easily implement solution for any case.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cascade.data as cdd\n",
    "import cascade.models as cdm\n",
    "import cascade.utils as cdu\n",
    "import cascade.meta as cde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.0-alpha'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cascade\n",
    "cascade.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation in general\n",
    "Cascade has basic validation building blocks as it has some specific validation solutions. In this section general cases will be explained.  \n",
    "In general one having a dataset can validate either all elements in the dataset one by one or a dataset as a whole. For these purposes Cascade has `PredicateValidator` and `AggregateValidator` classes. Let's see them on a real example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When everything is OK!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data. Tabular datasets will be used in the later section, let's now load the data for optical character recognition to demonstrate data validation features.\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We need to encapsulate the data using Cascade's default `Wrapper` to be able to use it later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_ds = cdd.Wrapper([(item, label) for item, label in zip(data['data'], data['target'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "  \n",
    "This dataset will give tuples of data and labels, which will be useful for training the model.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.]),\n",
       " 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the data we need to state our assumptions about it. Let's see the boundaries of values in images of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1., 16.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(data['data'], [0, 50, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the values are not lower than 0 and not higher than 16. Assume that we don't want future data to be outside these boundaries. We don't want new values silently breaking our pipeline.  \n",
    "Let's apply `PredicateValidator`. We pass our dataset and a callable that returns boolean value. If every item in the dataset passes this check the exception will not be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cascade.meta.validator.PredicateValidator"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_of_data = lambda x: x[0].max() <= 16 and x[0].min() >= 0\n",
    "cde.PredicateValidator(digits_ds, check_of_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This check is more strong - we don't want to add more labels by mistake, so let's check them similarly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cascade.meta.validator.PredicateValidator"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_of_label = lambda x: x[1] >= 0 and x[1] < 10\n",
    "cde.PredicateValidator(digits_ds, check_of_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validators are simple `Modifiers` that apply no transformation on dataset, so they can simply be chained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "validated_digits_ds = cde.PredicateValidator(digits_ds, check_of_data)\n",
    "validated_digits_ds = cde.PredicateValidator(validated_digits_ds, check_of_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to check the dataset as a whole. To demonstrate the mechanic let's check that dataset is big enough. Now our callable accepts the dataset and still returns boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cascade.meta.validator.AggregateValidator"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cde.AggregateValidator(digits_ds, lambda ds: len(ds) > 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want to check that our dataset (or pipeline) is **the same in different runs**? Can we do this by not specifying each parameter in the `AggregateValidator`?  \n",
    "Cascade has a special solution for this. It is `MetaValidator`.  \n",
    "`MetaValidator` works like the following. During first run it saves metadata into `./.cascade` folder. In the subsequent runs it checks whether some fields in meta changed and raises an exception if they did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as ./.cascade\\a3d6bd5de325ec32c9ce499b153b43bb.yml!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cascade.meta.meta_validator.MetaValidator"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cde.MetaValidator(digits_ds, meta_fmt='.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what values were saved by validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'cascade.data.dataset.Wrapper',\n",
       "  'type': 'dataset',\n",
       "  'len': 1797,\n",
       "  'obj_type': \"<class 'list'>\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_ds.get_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not much, but we can add to the meta everything we want before the first run of validator and it will be recorded in meta and checked.  \n",
    "Now let's simulate second run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cascade.meta.meta_validator.MetaValidator"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cde.MetaValidator(digits_ds, meta_fmt='.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ok, meta is unchanged. But what if we change the pipeline? MetaValidator works for unique pipelines. If we add new stage to it, it will make another record and will validate against it in the future. To identify pipelines It uses the list of dataset names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When everything is not OK\n",
    "What if our hypotheses are false due to some errors? Validators will raise `cascade.meta.DataValidationException` with the detailed description of what gone wrong where it possible. Let's see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of experiment, let's suppose that we don't wanna see zeros in labels. Let's check for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                            \r"
     ]
    },
    {
     "ename": "DataValidationException",
     "evalue": "Items [0, 10, 20, 30, 36, 48, 49, 55, 72, 78, 79, 101, 126, 130, 140, 150, 160, 166, 178, 179, 185, 202, 208, 209, 229, 252, 256, 266, 276, 286, 292, 304, 305, 311, 328, 334, 335, 357, 382, 386, 396, 406, 416, 422, 434, 435, 441, 458, 464, 465, 487, 512, 516, 526, 536, 546, 552, 564, 565, 571, 588, 594, 595, 617, 642, 646, 656, 666, 676, 682, 694, 695, 701, 718, 724, 725, 747, 772, 776, 786, 796, 806, 812, 824, 825, 831, 848, 854, 855, 877, 902, 915, 925, 935, 941, 957, 974, 980, 981, 1002, 1025, 1029, 1039, 1049, 1059, 1065, 1077, 1078, 1082, 1099, 1105, 1106, 1128, 1153, 1157, 1167, 1177, 1187, 1193, 1205, 1206, 1212, 1229, 1235, 1236, 1258, 1283, 1287, 1297, 1307, 1317, 1323, 1335, 1336, 1342, 1359, 1365, 1366, 1388, 1413, 1415, 1425, 1435, 1445, 1451, 1463, 1464, 1470, 1487, 1493, 1494, 1516, 1541, 1545, 1555, 1563, 1573, 1579, 1591, 1592, 1598, 1615, 1620, 1642, 1663, 1667, 1677, 1687, 1697, 1703, 1715, 1716, 1722, 1739, 1745, 1746, 1768, 1793] are not valid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataValidationException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\cascade\\cascade\\docs\\source\\examples\\data_validation.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/cascade/cascade/docs/source/examples/data_validation.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cde\u001b[39m.\u001b[39;49mPredicateValidator(digits_ds, \u001b[39mlambda\u001b[39;49;00m x: x[\u001b[39m1\u001b[39;49m] \u001b[39m!=\u001b[39;49m \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\cascade\\cascade\\meta\\validator.py:71\u001b[0m, in \u001b[0;36mPredicateValidator.__init__\u001b[1;34m(self, dataset, func, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m         bad_items\u001b[39m.\u001b[39mappend(i)\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(bad_items):\n\u001b[1;32m---> 71\u001b[0m     \u001b[39mraise\u001b[39;00m DataValidationException(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mItems \u001b[39m\u001b[39m{\u001b[39;00mbad_items\u001b[39m}\u001b[39;00m\u001b[39m are not valid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOK!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mDataValidationException\u001b[0m: Items [0, 10, 20, 30, 36, 48, 49, 55, 72, 78, 79, 101, 126, 130, 140, 150, 160, 166, 178, 179, 185, 202, 208, 209, 229, 252, 256, 266, 276, 286, 292, 304, 305, 311, 328, 334, 335, 357, 382, 386, 396, 406, 416, 422, 434, 435, 441, 458, 464, 465, 487, 512, 516, 526, 536, 546, 552, 564, 565, 571, 588, 594, 595, 617, 642, 646, 656, 666, 676, 682, 694, 695, 701, 718, 724, 725, 747, 772, 776, 786, 796, 806, 812, 824, 825, 831, 848, 854, 855, 877, 902, 915, 925, 935, 941, 957, 974, 980, 981, 1002, 1025, 1029, 1039, 1049, 1059, 1065, 1077, 1078, 1082, 1099, 1105, 1106, 1128, 1153, 1157, 1167, 1177, 1187, 1193, 1205, 1206, 1212, 1229, 1235, 1236, 1258, 1283, 1287, 1297, 1307, 1317, 1323, 1335, 1336, 1342, 1359, 1365, 1366, 1388, 1413, 1415, 1425, 1435, 1445, 1451, 1463, 1464, 1470, 1487, 1493, 1494, 1516, 1541, 1545, 1555, 1563, 1573, 1579, 1591, 1592, 1598, 1615, 1620, 1642, 1663, 1667, 1677, 1687, 1697, 1703, 1715, 1716, 1722, 1739, 1745, 1746, 1768, 1793] are not valid"
     ]
    }
   ],
   "source": [
    "cde.PredicateValidator(digits_ds, lambda x: x[1] != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exception all items causing the error are listed. This can be helpful to identify the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataValidationException",
     "evalue": "cascade.data.dataset.Wrapper fails on <function <lambda> at 0x00000174D9CDBBE0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataValidationException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\cascade\\cascade\\docs\\source\\examples\\data_validation.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/cascade/cascade/docs/source/examples/data_validation.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cde\u001b[39m.\u001b[39;49mAggregateValidator(digits_ds, \u001b[39mlambda\u001b[39;49;00m ds: \u001b[39mlen\u001b[39;49m(ds) \u001b[39m<\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\cascade\\cascade\\meta\\validator.py:47\u001b[0m, in \u001b[0;36mAggregateValidator.__init__\u001b[1;34m(self, dataset, func, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(dataset, func, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset):\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mraise\u001b[39;00m DataValidationException(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset)\u001b[39m}\u001b[39;00m\u001b[39m fails on \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOK!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mDataValidationException\u001b[0m: cascade.data.dataset.Wrapper fails on <function <lambda> at 0x00000174D9CDBBE0>"
     ]
    }
   ],
   "source": [
    "cde.AggregateValidator(digits_ds, lambda ds: len(ds) < 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exceptions provide info about what got wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's simulate the change in meta data of dataset. Let's manually change the length of it for example and see how `MetaValidator` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'cascade.data.dataset.Wrapper',\n",
       "  'type': 'dataset',\n",
       "  'len': 1000,\n",
       "  'obj_type': \"<class 'list'>\"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_ds._data = digits_ds._data[:1000]\n",
    "digits_ds.get_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of root[0]['len'] changed from 1797 to 1000.\n"
     ]
    },
    {
     "ename": "DataValidationException",
     "evalue": "{'values_changed': {\"root[0]['len']\": {'new_value': 1000, 'old_value': 1797}}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataValidationException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\cascade\\cascade\\docs\\source\\examples\\data_validation.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/cascade/cascade/docs/source/examples/data_validation.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cde\u001b[39m.\u001b[39;49mMetaValidator(digits_ds, meta_fmt\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.yml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\cascade\\cascade\\meta\\meta_validator.py:81\u001b[0m, in \u001b[0;36mMetaValidator.__init__\u001b[1;34m(self, dataset, root, meta_fmt)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(name):\n\u001b[0;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load(name)\n\u001b[1;32m---> 81\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check(meta)\n\u001b[0;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save(meta, name)\n",
      "File \u001b[1;32mc:\\cascade\\cascade\\meta\\meta_validator.py:96\u001b[0m, in \u001b[0;36mMetaValidator._check\u001b[1;34m(self, query_meta)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(diff):\n\u001b[0;32m     95\u001b[0m     \u001b[39mprint\u001b[39m(diff\u001b[39m.\u001b[39mpretty())\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mraise\u001b[39;00m DataValidationException(diff)\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOK!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mDataValidationException\u001b[0m: {'values_changed': {\"root[0]['len']\": {'new_value': 1000, 'old_value': 1797}}}"
     ]
    }
   ],
   "source": [
    "cde.MetaValidator(digits_ds, meta_fmt='.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provides detailed description of what values changed and how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of tables\n",
    "Validation of tabular data is more specific case and is more developed. For this purpose Cascade can use already made solutions in the familiar form of the `Validator`.  \n",
    "Now let's load tabular data for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data['data'], columns=data['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `TableDataset` - special container for `pandas.DataFrame`s in Cascade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cascade.utils.table_dataset.TableDataset\n",
       "      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_ds = cdu.TableDataset(t=df)\n",
    "iris_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "For the purpose of tabular data validation Cascade uses Pandera. The workflow is simple - you define schema of the table and the checks that should be made. Then you run `PaSchemaValidator` and that's all!  \n",
    "For the documentation of Pandera's classes, please see: [pandera docs](https://pandera.readthedocs.io/en/stable/index.html).\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add all columns and check that all values are greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pa.DataFrameSchema({\n",
    "    \"sepal length (cm)\": pa.Column(float, checks=pa.Check.gt(0)),\n",
    "    \"sepal width (cm)\": pa.Column(float, checks=pa.Check.gt(0)),\n",
    "    \"petal length (cm)\": pa.Column(float, checks=pa.Check.gt(0)),\n",
    "    \"petal width (cm)\": pa.Column(float, checks=pa.Check.gt(0)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cascade.utils.pa_schema_validator.PaSchemaValidator"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdu.PaSchemaValidator(iris_ds, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future uses we can save schema to yaml and use Validator with the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.to_yaml('./iris_schema.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cascade.utils.pa_schema_validator.PaSchemaValidator"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdu.PaSchemaValidator(iris_ds, './iris_schema.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually violate our assumption and see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_ds._table['sepal length (cm)'] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SchemaDefinitionError",
     "evalue": "Schema representation must be a mapping.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSchemaDefinitionError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\cascade\\cascade\\docs\\source\\examples\\data_validation.ipynb Cell 54\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/cascade/cascade/docs/source/examples/data_validation.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cdu\u001b[39m.\u001b[39;49mPaSchemaValidator(iris_ds, \u001b[39m'\u001b[39;49m\u001b[39m./iris_schema.yaml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\cascade\\cascade\\utils\\pa_schema_validator.py:23\u001b[0m, in \u001b[0;36mPaSchemaValidator.__init__\u001b[1;34m(self, dataset, schema, *args, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, dataset, schema, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m        For more details on schemas see pandera's documentation.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(dataset, \u001b[39m*\u001b[39margs, func\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate(x, schema), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\cascade\\cascade\\meta\\validator.py:46\u001b[0m, in \u001b[0;36mAggregateValidator.__init__\u001b[1;34m(self, dataset, func, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, dataset: Dataset, func: Callable[[Dataset], \u001b[39mbool\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(dataset, func, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset):\n\u001b[0;32m     47\u001b[0m         \u001b[39mraise\u001b[39;00m DataValidationException(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset)\u001b[39m}\u001b[39;00m\u001b[39m fails on \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\cascade\\cascade\\utils\\pa_schema_validator.py:23\u001b[0m, in \u001b[0;36mPaSchemaValidator.__init__.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, dataset, schema, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m        For more details on schemas see pandera's documentation.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(dataset, \u001b[39m*\u001b[39margs, func\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(x, schema), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\cascade\\cascade\\utils\\pa_schema_validator.py:29\u001b[0m, in \u001b[0;36mPaSchemaValidator._validate\u001b[1;34m(ds, schema)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(schema) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m         schema \u001b[39m=\u001b[39m paio\u001b[39m.\u001b[39;49mfrom_yaml(schema)\n\u001b[0;32m     30\u001b[0m     schema\u001b[39m.\u001b[39mvalidate(ds\u001b[39m.\u001b[39m_table)\n\u001b[0;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m SchemaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\илья\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandera\\io.py:273\u001b[0m, in \u001b[0;36mfrom_yaml\u001b[1;34m(yaml_schema)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mOSError\u001b[39;00m):\n\u001b[0;32m    272\u001b[0m     serialized_schema \u001b[39m=\u001b[39m yaml\u001b[39m.\u001b[39msafe_load(yaml_schema)\n\u001b[1;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m _deserialize_schema(serialized_schema)\n",
      "File \u001b[1;32mc:\\Users\\илья\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandera\\io.py:217\u001b[0m, in \u001b[0;36m_deserialize_schema\u001b[1;34m(serialized_schema)\u001b[0m\n\u001b[0;32m    214\u001b[0m serialized_schema \u001b[39m=\u001b[39m serialized_schema \u001b[39mif\u001b[39;00m serialized_schema \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(serialized_schema, Mapping):\n\u001b[1;32m--> 217\u001b[0m     \u001b[39mraise\u001b[39;00m pandera\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mSchemaDefinitionError(\n\u001b[0;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSchema representation must be a mapping.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    221\u001b[0m columns \u001b[39m=\u001b[39m serialized_schema\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m index \u001b[39m=\u001b[39m serialized_schema\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mSchemaDefinitionError\u001b[0m: Schema representation must be a mapping."
     ]
    }
   ],
   "source": [
    "cdu.PaSchemaValidator(iris_ds, './iris_schema.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained large traceback which shows which values violate our assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data validation is an important part of any established ML-pipeline. Simple checks can speed up problem identification. By using Cascade one can easily develop own dataset checks and use already made solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See also:\n",
    "- [Documentation](https://oxid15.github.io/cascade/)\n",
    "- [Key concepts](https://oxid15.github.io/cascade/concepts.html)\n",
    "- [Code reference](https://oxid15.github.io/cascade/modules.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87f4f7b13864d85c4a2c9cc8379c2fc28050103468031879c35c8b8ef7dd6990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
